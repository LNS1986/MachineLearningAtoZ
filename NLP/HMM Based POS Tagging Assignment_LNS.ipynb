{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')], [('Rudolph', 'NOUN'), ('Agnew', 'NOUN'), (',', '.'), ('55', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), ('and', 'CONJ'), ('former', 'ADJ'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Consolidated', 'NOUN'), ('Gold', 'NOUN'), ('Fields', 'NOUN'), ('PLC', 'NOUN'), (',', '.'), ('was', 'VERB'), ('named', 'VERB'), ('*-1', 'X'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('British', 'ADJ'), ('industrial', 'ADJ'), ('conglomerate', 'NOUN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "print(nltk_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3718\n",
      "196\n",
      "[[('Bank', 'NOUN'), ('of', 'ADP'), ('New', 'NOUN'), ('England', 'NOUN'), (\"'s\", 'PRT'), ('shares', 'NOUN'), ('are', 'VERB'), ('traded', 'VERB'), ('*-1', 'X'), ('on', 'ADP'), ('the', 'DET'), ('New', 'NOUN'), ('York', 'NOUN'), ('Stock', 'NOUN'), ('Exchange', 'NOUN'), ('.', '.')], [('$', '.'), ('130', 'NUM'), ('million', 'NUM'), ('*U*', 'X'), ('of', 'ADP'), ('general', 'ADJ'), ('obligation', 'NOUN'), ('distributable', 'ADJ'), ('state', 'NOUN'), ('aid', 'NOUN'), ('bonds', 'NOUN'), ('due', 'ADJ'), ('1991-2000', 'NUM'), ('and', 'CONJ'), ('2009', 'NUM'), (',', '.'), ('tentatively', 'ADV'), ('priced', 'VERB'), ('*', 'X'), ('by', 'ADP'), ('a', 'DET'), ('Chemical', 'NOUN'), ('Securities', 'NOUN'), ('Inc.', 'NOUN'), ('group', 'NOUN'), ('*', 'X'), ('to', 'PRT'), ('yield', 'VERB'), ('from', 'ADP'), ('6.20', 'NUM'), ('%', 'NOUN'), ('in', 'ADP'), ('1991', 'NUM'), ('to', 'PRT'), ('7.272', 'NUM'), ('%', 'NOUN'), ('in', 'ADP'), ('2009', 'NUM'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# splitting into train and test\n",
    "random.seed(1234)\n",
    "train_set, test_set = train_test_split(nltk_data, test_size=0.05,random_state=42)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "print(train_set[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tagges Words 95589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Bank', 'of', 'New', 'England', \"'s\", 'shares', 'are', 'traded', '*-1', 'on']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of tagged words\n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]\n",
    "print('Total Tagges Words',len(train_tagged_words))\n",
    "\n",
    "# tokens \n",
    "tokens = [pair[0] for pair in train_tagged_words]\n",
    "\n",
    "#Check some sentence data\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interface for converting POS tags from various treebanks to the universal tagset of Petrov, Das, & McDonald.\n",
    "\n",
    "The tagset consists of the following 12 coarse tags:\n",
    "    \n",
    "    VERB - verbs (all tenses and modes)\n",
    "    NOUN - nouns (common and proper)\n",
    "    PRON - pronouns\n",
    "    ADJ - adjectives\n",
    "    ADV - adverbs\n",
    "    ADP - adpositions (prepositions and postpositions)\n",
    "    CONJ - conjunctions\n",
    "    DET - determiners\n",
    "    NUM - cardinal numbers\n",
    "    PRT - particles or other function words\n",
    "    X - other: foreign words, typos, abbreviations\n",
    "    . - punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary\n",
    "V = set(tokens)\n",
    "print(len(V))\n",
    "\n",
    "# number of tags\n",
    "T = set([pair[1] for pair in train_tagged_words])\n",
    "len(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emission Probabilities\n",
    "# computing P(w/t) and storing in T x V matrix\n",
    "t = len(T)\n",
    "v = len(V)\n",
    "w_given_t = np.zeros((t, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12109)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the train set shape\n",
    "w_given_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word given tag: Emission Probability\n",
    "from collections import Counter\n",
    "tags = [pair[1] for pair in train_tagged_words]\n",
    "tag_counts = Counter(tags)\n",
    "\n",
    "all_Tags = list(set([pair[1] for pair in train_tagged_words]))\n",
    "tag_word_dict = dict()\n",
    "for t in all_Tags:\n",
    "    tag_word_dict.update({t:[pair[0] for pair in train_tagged_words if pair[1]== t]})\n",
    "\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    count_tag = tag_counts.get(tag)\n",
    "    tag_list = tag_word_dict.get(tag)\n",
    "    count_w_given_tag = tag_list.count(word)\n",
    "    return (count_w_given_tag, count_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(T), len(T)), dtype='float32')\n",
    "for i, t1 in enumerate(list(T)):\n",
    "    for j, t2 in enumerate(list(T)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (12, 12)\n",
      "Transition probs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VERB</th>\n",
       "      <th>PRT</th>\n",
       "      <th>ADV</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>X</th>\n",
       "      <th>DET</th>\n",
       "      <th>.</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>PRON</th>\n",
       "      <th>NUM</th>\n",
       "      <th>ADP</th>\n",
       "      <th>CONJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.169189</td>\n",
       "      <td>0.031121</td>\n",
       "      <td>0.082577</td>\n",
       "      <td>0.110904</td>\n",
       "      <td>0.218005</td>\n",
       "      <td>0.133101</td>\n",
       "      <td>0.035312</td>\n",
       "      <td>0.064649</td>\n",
       "      <td>0.036244</td>\n",
       "      <td>0.022817</td>\n",
       "      <td>0.090493</td>\n",
       "      <td>0.005588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.402746</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.010134</td>\n",
       "      <td>0.242563</td>\n",
       "      <td>0.013403</td>\n",
       "      <td>0.100360</td>\n",
       "      <td>0.041517</td>\n",
       "      <td>0.086303</td>\n",
       "      <td>0.018960</td>\n",
       "      <td>0.058516</td>\n",
       "      <td>0.021576</td>\n",
       "      <td>0.002288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.344518</td>\n",
       "      <td>0.013621</td>\n",
       "      <td>0.081063</td>\n",
       "      <td>0.030897</td>\n",
       "      <td>0.023588</td>\n",
       "      <td>0.067110</td>\n",
       "      <td>0.136877</td>\n",
       "      <td>0.130233</td>\n",
       "      <td>0.015615</td>\n",
       "      <td>0.030565</td>\n",
       "      <td>0.119601</td>\n",
       "      <td>0.006312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.147978</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.016884</td>\n",
       "      <td>0.264632</td>\n",
       "      <td>0.029136</td>\n",
       "      <td>0.013310</td>\n",
       "      <td>0.239179</td>\n",
       "      <td>0.012289</td>\n",
       "      <td>0.004923</td>\n",
       "      <td>0.009627</td>\n",
       "      <td>0.176275</td>\n",
       "      <td>0.041936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.203633</td>\n",
       "      <td>0.185787</td>\n",
       "      <td>0.025175</td>\n",
       "      <td>0.061345</td>\n",
       "      <td>0.076482</td>\n",
       "      <td>0.055131</td>\n",
       "      <td>0.163799</td>\n",
       "      <td>0.016571</td>\n",
       "      <td>0.056087</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>0.142925</td>\n",
       "      <td>0.010357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.038387</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.012313</td>\n",
       "      <td>0.640029</td>\n",
       "      <td>0.045509</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.204973</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0.021970</td>\n",
       "      <td>0.009054</td>\n",
       "      <td>0.000483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.088505</td>\n",
       "      <td>0.002339</td>\n",
       "      <td>0.052078</td>\n",
       "      <td>0.223152</td>\n",
       "      <td>0.026623</td>\n",
       "      <td>0.173502</td>\n",
       "      <td>0.093812</td>\n",
       "      <td>0.044972</td>\n",
       "      <td>0.065389</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.091114</td>\n",
       "      <td>0.057924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.012342</td>\n",
       "      <td>0.010861</td>\n",
       "      <td>0.004608</td>\n",
       "      <td>0.696725</td>\n",
       "      <td>0.021392</td>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.065328</td>\n",
       "      <td>0.066645</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.020405</td>\n",
       "      <td>0.078986</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.480901</td>\n",
       "      <td>0.012223</td>\n",
       "      <td>0.033995</td>\n",
       "      <td>0.211230</td>\n",
       "      <td>0.092819</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>0.041253</td>\n",
       "      <td>0.074866</td>\n",
       "      <td>0.008021</td>\n",
       "      <td>0.007257</td>\n",
       "      <td>0.022918</td>\n",
       "      <td>0.004966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.027951</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.355338</td>\n",
       "      <td>0.206661</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.118347</td>\n",
       "      <td>0.034196</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.184062</td>\n",
       "      <td>0.034790</td>\n",
       "      <td>0.013381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.008240</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>0.321776</td>\n",
       "      <td>0.034029</td>\n",
       "      <td>0.326378</td>\n",
       "      <td>0.039486</td>\n",
       "      <td>0.105297</td>\n",
       "      <td>0.069128</td>\n",
       "      <td>0.062921</td>\n",
       "      <td>0.017228</td>\n",
       "      <td>0.000856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.153918</td>\n",
       "      <td>0.004693</td>\n",
       "      <td>0.055842</td>\n",
       "      <td>0.349132</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.121539</td>\n",
       "      <td>0.034256</td>\n",
       "      <td>0.116847</td>\n",
       "      <td>0.058658</td>\n",
       "      <td>0.042234</td>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.000469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          VERB       PRT       ADV      NOUN         X       DET         .  \\\n",
       "VERB  0.169189  0.031121  0.082577  0.110904  0.218005  0.133101  0.035312   \n",
       "PRT   0.402746  0.001635  0.010134  0.242563  0.013403  0.100360  0.041517   \n",
       "ADV   0.344518  0.013621  0.081063  0.030897  0.023588  0.067110  0.136877   \n",
       "NOUN  0.147978  0.043832  0.016884  0.264632  0.029136  0.013310  0.239179   \n",
       "X     0.203633  0.185787  0.025175  0.061345  0.076482  0.055131  0.163799   \n",
       "DET   0.038387  0.000241  0.012313  0.640029  0.045509  0.005311  0.017986   \n",
       ".     0.088505  0.002339  0.052078  0.223152  0.026623  0.173502  0.093812   \n",
       "ADJ   0.012342  0.010861  0.004608  0.696725  0.021392  0.005101  0.065328   \n",
       "PRON  0.480901  0.012223  0.033995  0.211230  0.092819  0.009549  0.041253   \n",
       "NUM   0.017544  0.027951  0.002974  0.355338  0.206661  0.003271  0.118347   \n",
       "ADP   0.008240  0.001498  0.013162  0.321776  0.034029  0.326378  0.039486   \n",
       "CONJ  0.153918  0.004693  0.055842  0.349132  0.007977  0.121539  0.034256   \n",
       "\n",
       "           ADJ      PRON       NUM       ADP      CONJ  \n",
       "VERB  0.064649  0.036244  0.022817  0.090493  0.005588  \n",
       "PRT   0.086303  0.018960  0.058516  0.021576  0.002288  \n",
       "ADV   0.130233  0.015615  0.030565  0.119601  0.006312  \n",
       "NOUN  0.012289  0.004923  0.009627  0.176275  0.041936  \n",
       "X     0.016571  0.056087  0.002709  0.142925  0.010357  \n",
       "DET   0.204973  0.003742  0.021970  0.009054  0.000483  \n",
       ".     0.044972  0.065389  0.080500  0.091114  0.057924  \n",
       "ADJ   0.066645  0.000658  0.020405  0.078986  0.016949  \n",
       "PRON  0.074866  0.008021  0.007257  0.022918  0.004966  \n",
       "NUM   0.034196  0.001487  0.184062  0.034790  0.013381  \n",
       "ADP   0.105297  0.069128  0.062921  0.017228  0.000856  \n",
       "CONJ  0.116847  0.058658  0.042234  0.054435  0.000469  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))\n",
    "\n",
    "# check the shape\n",
    "print('Shape:',tags_df.shape)\n",
    "\n",
    "print('Transition probs:')\n",
    "tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_out = word_given_tag(words[key], tag)\n",
    "            emission_p = emission_out[0]/emission_out[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_set for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5087"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  43.12033820152283\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken in seconds: \", difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged test words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('For', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('Agency', 'NOUN'),\n",
       " ('for', 'ADP'),\n",
       " ('International', 'NOUN')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Tagged test words')\n",
    "tagged_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  43.12033820152283\n",
      "Total wrongly tagged words 385\n",
      "Distinct Word Count 347\n",
      "0.92431688618046\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken in seconds: \", difference)\n",
    "test_taggings = [tup for sent in test_set for tup in sent]\n",
    "wronglytagged_words =[]\n",
    "for i in range(len(tagged_seq)):\n",
    "    if test_taggings[i][1] != tagged_seq[i][1]:\n",
    "        wronglytagged_words.append((tagged_seq[i],test_taggings[i]))\n",
    "print('Total wrongly tagged words',len(wronglytagged_words))\n",
    "distinct_words = list(set([pair[0] for pair in wronglytagged_words]))\n",
    "\n",
    "print('Distinct Word Count',len(distinct_words))\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_taggings) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('trade', 'VERB'), ('trade', 'NOUN')),\n",
       " (('Overseas', 'VERB'), ('Overseas', 'NOUN')),\n",
       " (('Private', 'ADJ'), ('Private', 'NOUN')),\n",
       " (('pre-1917', 'VERB'), ('pre-1917', 'ADJ')),\n",
       " (('Unemployment', 'VERB'), ('Unemployment', 'NOUN'))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wronglytagged_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the problem of unknown words\n",
    "\n",
    "    VERB - verbs (all tenses and modes)\n",
    "    NOUN - nouns (common and proper)\n",
    "    PRON - pronouns\n",
    "    ADJ - adjectives\n",
    "    ADV - adverbs\n",
    "    ADP - adpositions (prepositions and postpositions)\n",
    "    CONJ - conjunctions\n",
    "    DET - determiners\n",
    "    NUM - cardinal numbers\n",
    "    PRT - particles or other function words\n",
    "    X - other: foreign words, typos, abbreviations\n",
    "    . - punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the word is number or not\n",
    "def isDigit(word):\n",
    "    try:\n",
    "        x = float(word)\n",
    "        isDigit = True\n",
    "    except:\n",
    "        isDigit = False\n",
    "\n",
    "    return isDigit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Few morphological rules used to assign unknown word tokens\n",
    "# Numbers\n",
    "nums = ['one','two','three','four','five','six','seven','eight','nine','ten']\n",
    "\n",
    "# Suffixes\n",
    "noun_suffix = [\"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\",\"action\", \"age\", \"ance\", \"cy\",\"an\", \n",
    "               \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\"]\n",
    "\n",
    "verb_suffix = [ \"ize\",\"ing\",\"ed\",\"ate\", \"ify\", \"ise\"]\n",
    "adjective_suffix = [\"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ous\",\"able\", \"ese\", \"ful\", \"i\", \"ian\"]\n",
    "adverb_suffix = [\"ward\", \"wards\", \"wise\",\"ly\",\"though\",\"about\",\"above\",\"as\",\"out\",\"further\",\"due\",\"most\"]\n",
    "\n",
    "# Punctuation characters\n",
    "punct = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for Lexicon and Rule Based Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLexiconTag():\n",
    "    # Rule Based Tagging\n",
    "    patterns = [\n",
    "        (r'.*[-a-z]ing$', 'ADJ'),\n",
    "        (r'.*[-a-z]ed$', 'ADJ'),\n",
    "        (r'.*[-a-z]$', 'NOUN'),\n",
    "        (r'.*ing$', 'VERB'),              # gerund\n",
    "        (r'.*ed$', 'VERB'),               # past tense\n",
    "        (r'.*es$', 'VERB'),               # 3rd singular present\n",
    "        (r'.*ould$', 'VERB'),             # modals\n",
    "        (r'^\\'s$','VERB'),\n",
    "        (r'^\\'S$','VERB'),\n",
    "        (r'^[A-Za-z]\\.[A-Za-z]\\.$','NOUN'),\n",
    "        (r'^[A-Za-z]\\.$','NOUN'),\n",
    "        (r'^[0-9]s$', 'NUM'), # cardinal numbers\n",
    "        (r'.*\\'s$', 'NOUN'),              # possessive nouns        \n",
    "        (r'.*s$', 'NOUN'),               # plural nouns\n",
    "        (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'), # cardinal numbers\n",
    "        (r'\\*[A-Za-z]\\*','X'),             #other: foreign words, typos, abbreviations\n",
    "        (r'^\\*-','X'),\n",
    "        (r'.*', 'NOUN')                   # nouns\n",
    "    ]\n",
    "\n",
    "    # rule based tagger\n",
    "    rule_based_tagger = nltk.RegexpTagger(patterns)\n",
    "\n",
    "    # lexicon backed up by the rule-based tagger\n",
    "    lexicon_tagger = nltk.UnigramTagger(train_set, backoff=rule_based_tagger)\n",
    "    \n",
    "    lexicon_tagger = nltk.BigramTagger(train_set, backoff=lexicon_tagger)\n",
    "    \n",
    "    return lexicon_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('20s', 'NOUN')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_tag = getLexiconTag()\n",
    "lexicon_tag.tag(['*-123'])\n",
    "lexicon_tag.tag(['exists'])\n",
    "lexicon_tag.tag(['20s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_Unknown_Word(word):\n",
    "    \"\"\"\n",
    "    Assign unknown word as per customised tokens\n",
    "    \"\"\"\n",
    "    # Digits - Check if al characters are digit\n",
    "    if all(char.isdigit() for char in word) or isDigit(word) or (word.lower() in nums):\n",
    "        return \"NUM\"\n",
    "\n",
    "    # Punctuation\n",
    "    elif all(char in punct for char in word):\n",
    "        return \".\"\n",
    "\n",
    "    # Upper-case\n",
    "    elif all(char.isupper() for char in word) or word[0].isupper():\n",
    "        return \"NOUN\"\n",
    "\n",
    "    # Nouns\n",
    "    elif any(word.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"NOUN\"\n",
    "\n",
    "    # Verbs\n",
    "    elif any(word.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"VERB\"\n",
    "\n",
    "    # Adjectives\n",
    "    elif any(word.endswith(suffix) for suffix in adjective_suffix):\n",
    "        return \"ADJ\"\n",
    "\n",
    "    # Adverbs\n",
    "    elif any(word.endswith(suffix) for suffix in adverb_suffix):\n",
    "        return \"ADV\"\n",
    "    \n",
    "    # Adverbs\n",
    "    elif word in ['that']:\n",
    "        return \"DET\"\n",
    "\n",
    "    return \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assign_Unknown_Word('private')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def ViterbiForUnknownWords(words, train_bag = train_tagged_words, debug=False):\n",
    "    #Train as for Lexicon Tag \n",
    "    lexicon_tag = getLexiconTag()\n",
    "    \n",
    "    \n",
    "    state = []\n",
    "    unknown_words=[]\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    # print(T)\n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_out = word_given_tag(words[key], tag)\n",
    "            emission_p = emission_out[0]/emission_out[1]\n",
    "            state_probability = emission_p * transition_p \n",
    "            p.append(state_probability)\n",
    "            #if word == 'purchasing':\n",
    "            #    debug = True\n",
    "            #    print(emission_out)\n",
    "            #   print(transition_p)\n",
    "            #else:\n",
    "            #    debug = False\n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        if pmax!=0.0:\n",
    "            state_max = T[p.index(pmax)]\n",
    "            if debug:\n",
    "                print('All Probs:',p)\n",
    "                print('Max Probs',p.index(pmax))\n",
    "                print('Predicted Tag:',state_max)\n",
    "            #print(state_max)\n",
    "        else:\n",
    "            if debug:\n",
    "                print('Customized Method Block')\n",
    "            # get tagging for unknown words\n",
    "            state_max = assign_Unknown_Word(word)\n",
    "            # state_max = lexicon_tag.tag(tokens=[word])[0][1]\n",
    "            if state_max =='UNK':\n",
    "                state_max = lexicon_tag.tag(tokens=[word])[0][1]\n",
    "                if debug:\n",
    "                    print('Lexicon Block:',state_max)\n",
    "            \n",
    "            #List To hold the unknown words and their tag\n",
    "            unknown_words.append((word,state_max))        \n",
    "                \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state)),unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "tagged_seq_new,unknown_words = ViterbiForUnknownWords(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Probs: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.400381436606114e-06, 0.0, 0.0, 0.0, 0.0]\n",
      "Max Probs 7\n",
      "Predicted Tag: ADJ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('Palestinian', 'ADJ')], [])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_given_tag('purchasing','VERB')\n",
    "#lexicon_tag.evaluate(test_set)\n",
    "# lexicon_tag.tag(tokens=['exists'])\n",
    "ViterbiForUnknownWords(['Palestinian'],debug=True)\n",
    "# [pair for pair in train_tagged_words if pair[0] == 'purchasing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A - Accuracy Viterbi Along with Lexicon Tagging and customised method to understand nound, proverb, adjective based on language knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  48.14597272872925\n",
      "Total wrongly tagged words 210\n",
      "Distinct Word Count 180\n",
      "0.9587183015529782\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken in seconds: \", difference)\n",
    "test_taggings = [tup for sent in test_set for tup in sent]\n",
    "wronglytagged_words_new =[]\n",
    "for i in range(len(tagged_seq_new)):\n",
    "    if test_taggings[i][1] != tagged_seq_new[i][1]:\n",
    "        wronglytagged_words_new.append((tagged_seq_new[i],test_taggings[i]))\n",
    "print('Total wrongly tagged words',len(wronglytagged_words_new))\n",
    "distinct_words_new = list(set([pair[0] for pair in wronglytagged_words_new]))\n",
    "\n",
    "print('Distinct Word Count',len(distinct_words_new))\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq_new, test_taggings) if i == j] \n",
    "accuracy_new = len(check)/len(tagged_seq_new)\n",
    "print(accuracy_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B - Accuracy of only Lexicon and Rule Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9530174955769609"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getLexiconTag().evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VERB', 'PRT', 'ADV', 'NOUN', 'X', 'DET', '.', 'ADJ', 'PRON', 'NUM', 'ADP', 'CONJ']\n"
     ]
    }
   ],
   "source": [
    "# Check Words which are not correctly tagged - Testing\n",
    "print(list(set([pair[1] for pair in train_tagged_words])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('trade', 'VERB'), ('trade', 'NOUN')),\n",
       " (('Private', 'ADJ'), ('Private', 'NOUN')),\n",
       " (('slate', 'VERB'), ('slate', 'NOUN')),\n",
       " (('homeless', 'ADJ'), ('homeless', 'NOUN')),\n",
       " (('test', 'VERB'), ('test', 'NOUN')),\n",
       " (('bearing', 'VERB'), ('bearing', 'NOUN')),\n",
       " (('work', 'VERB'), ('work', 'NOUN')),\n",
       " (('force', 'VERB'), ('force', 'NOUN')),\n",
       " (('average', 'ADJ'), ('average', 'NOUN')),\n",
       " (('air', 'VERB'), ('air', 'NOUN')),\n",
       " (('word-processing', 'VERB'), ('word-processing', 'NOUN')),\n",
       " (('declines', 'VERB'), ('declines', 'NOUN')),\n",
       " (('covers', 'VERB'), ('covers', 'NOUN')),\n",
       " (('pins', 'VERB'), ('pins', 'NOUN')),\n",
       " (('estimates', 'VERB'), ('estimates', 'NOUN')),\n",
       " (('slowing', 'VERB'), ('slowing', 'NOUN')),\n",
       " (('fledgling', 'VERB'), ('fledgling', 'NOUN')),\n",
       " (('talk', 'VERB'), ('talk', 'NOUN')),\n",
       " (('Palestinian', 'ADJ'), ('Palestinian', 'NOUN')),\n",
       " (('half-hour', 'ADJ'), ('half-hour', 'NOUN')),\n",
       " (('buy', 'VERB'), ('buy', 'NOUN')),\n",
       " (('sell', 'VERB'), ('sell', 'NOUN')),\n",
       " (('centers', 'VERB'), ('centers', 'NOUN')),\n",
       " (('focus', 'VERB'), ('focus', 'NOUN')),\n",
       " (('playing', 'VERB'), ('playing', 'NOUN')),\n",
       " (('monopoly', 'ADV'), ('monopoly', 'NOUN')),\n",
       " (('estimates', 'VERB'), ('estimates', 'NOUN')),\n",
       " (('future', 'ADJ'), ('future', 'NOUN')),\n",
       " (('export', 'VERB'), ('export', 'NOUN')),\n",
       " (('cable', 'ADJ'), ('cable', 'NOUN')),\n",
       " (('cut', 'VERB'), ('cut', 'NOUN')),\n",
       " (('acid', 'ADJ'), ('acid', 'NOUN')),\n",
       " (('attack', 'VERB'), ('attack', 'NOUN')),\n",
       " (('target', 'VERB'), ('target', 'NOUN')),\n",
       " (('total', 'ADJ'), ('total', 'NOUN')),\n",
       " (('Soviet', 'ADJ'), ('Soviet', 'NOUN')),\n",
       " (('cost', 'VERB'), ('cost', 'NOUN')),\n",
       " (('use', 'VERB'), ('use', 'NOUN'))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wronglytagged_words_new if w[1][1]=='NOUN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wrongly tagged words by Vanila Viterbi: 385\n",
      "Distinct Word Count which are wrongly tagged by Vanila Viterbi: 347\n",
      "Accuracy by Vanila Viterbi: 0.92431688618046\n"
     ]
    }
   ],
   "source": [
    "print('Total wrongly tagged words by Vanila Viterbi:',len(wronglytagged_words))\n",
    "print('Distinct Word Count which are wrongly tagged by Vanila Viterbi:',len(distinct_words))\n",
    "print('Accuracy by Vanila Viterbi:',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wrongly tagged words after correcting Unknown Words: 210\n",
      "Distinct Word Count after correcting Unknown Words: 180\n",
      "Accuracy by Vanila Viterbi: 0.9587183015529782\n"
     ]
    }
   ],
   "source": [
    "print('Total wrongly tagged words after correcting Unknown Words:',len(wronglytagged_words_new))\n",
    "print('Distinct Word Count after correcting Unknown Words:',len(distinct_words_new))\n",
    "print('Accuracy by Vanila Viterbi:',accuracy_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unknown Words: 307\n"
     ]
    }
   ],
   "source": [
    "print('Total Unknown Words:',len(unknown_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctly_tagged_unknown_words=[]\n",
    "for unk in unknown_words:\n",
    "    for tst in wronglytagged_words:\n",
    "        if unk[0] == tst[1][0] and tst[1][1]==unk[1]:\n",
    "            correctly_tagged_unknown_words.append(unk)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 385 number of unknown words, modfified Viterbi has correctly POS tagged 207 numbers of words\n"
     ]
    }
   ],
   "source": [
    "print('Out of {0} number of unknown words, modfified Viterbi has correctly POS tagged {1} numbers of words'\n",
    "      .format(len(wronglytagged_words),len(correctly_tagged_unknown_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly Tagged Unknownd Words are:\n",
      " ['Overseas', 'Unemployment', 'paycheck', 'reasonably', 'Tokio', 'protocols', 'preventative', '20.5', 'acquirers', '154.2', 'chalk', 'schoolchildren', 'Catch-22', 'emigres', '*T*-133', '1953', '1955', 'addiction', '1,200', 'Mogavero', 'Piscataway', '*T*-253', 'reds', 'Rhone', '133.7', '94', 'octogenarians', '*T*-222', 'belfries', 'Anglia', '405', '*T*-102', 'Communication', 'importer', '*T*-128', '*T*-129', 'mail', 'alternatively', 'intrusions', 'Sloan', '43.875', 'reluctance', 'lobbies', '133.8', '2.47', '7.4', '2.30', 'modification', '*-130', '13.625', 'blindfold', 'Anything', 'Northampton', 'protein', 'Fundamentalists', 'bags', 'workplace', 'Legend', 'Iran-Contra', 'Heidelberg', 'approaches', 'college-bowl', 'competitions', 'coffee', 'Velcro', 'platinum', 'platinum', 'Johnson-era', 'noncompetitively', 'Wilfred', 'Corrigan', 'derivatives', 'binders', '93.9', '1.19', '92.9', '1.18', 'Demand', 'Berson', 'longevity', 'rarely', 'deplorable', 'insurgents', '16.5', 'technically', 'limitation', 'leash', '777', '1.20', '1.14', 'skill', '2163.2', 'improbable', 'shop', '2-8', 'A.C.', 'Nielsen', 'supportive', 'element', 'Breeden', 'breakers', '*T*-215', '87.5', '38.875', 'noodles', 'pasta', 'Clive', 'Smaby', 'Smaby', 'Smaby', 'Smaby', 'properties', 'INTER-TEL', 'Gelles', 'Wertheim', 'Schroder', 'suitors', 'inverse', '66.5', 'Hazell', 'auditor', 'seconds', 'fancy', 'strategic', 'stock-price', '7.422', 'crunch', 'crunch', 'crunch', 'crunch', 'crunch', 'crunch', 'crunch', 'crunch', 'crunch', 'bang', 'bang', 'bang', 'bang', 'bang', 'bang', 'bang', 'bang', 'bang', 'obvious', 'exit', 'prayer', 'millionaires', 'replacement-car', 'rentals', 'accidents', 'microprocessor', 'megabytes', '5,699', '6,799', 'storm', 'Citizens', 'Peninsula', '*T*-156', 'developments', 'compilation', 'actor', 'inheritor', 'Charlie', 'volunteer', 'insolvency', 'fan', '50.1', 'DiLoreto', 'container', 'Delmont', 'FreudToy', 'pillow', 'likeness', 'Sigmund', 'Freud', '24.95', 'tool', '1.82', '84.29', 'longtime', 'scholar', 'enormously', 'science', 'Walkman', 'IRAs', 'TXO', 'trip', 'mindful', 'mesothelioma', 'asbestosis', '16.9', '17.4', '18.6', 'Tokyu', 'verge', 'accidentally', 'jam', 'cocoa', 'goodies', 'reins', '1.50', 'I.', 'platinum', 'platinum', '5.70', '494.50']\n"
     ]
    }
   ],
   "source": [
    "print('Correctly Tagged Unknownd Words are:\\n', list([i[0] for i in correctly_tagged_unknown_words ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrectly_tagged_unknown_words=[]\n",
    "for corr in unknown_words:\n",
    "    for unk in wronglytagged_words:\n",
    "        if unk[1][0]==corr[0] and unk[1][1]!=corr[1]:\n",
    "            incorrectly_tagged_unknown_words.append(unk[1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 307 number of unknown words, modfified Viterbi has unable to correctly POS tagged 58 numbers of words.\n",
      "Incorrectly Tagged Unknownd Words are:\n",
      " ['pre-1917', 'Though', 'slate', 'cross-border', 'safe', 'pre-existing', 'American-style', 'sometimes-exhausting', 'manmade-fiber', 'low-cost', 'supplemental', 'anti-drug', 'computer-generated', 'certified', 'void', 'Muzzling', 'side-crash', 'word-processing', 'multinational', 'wheel-loader', 'desultory', 'herself', 'non-core', 'twice', 'apparent', 'razor-thin', 'fledgling', 'triple', 'Washington-based', 'high-rise', 'pre-cooked', 'entertaining', 'confrontational', 'shareholder-rights', 'unwanted', 'low-tech', 'cumbersome', 'day-care', 'monopoly', '20s', '30s', 'computer-system-design', 'more-advanced', '100-megabyte', 'floral', '70-a-share', 'price-support', 'direct-investment', 'avid', 'cable', 'seven-million-ton', 'do-it-yourself', 'raring', 'lap-shoulder', 'rear', 'malignant', 'war-rationed', 'improper']\n"
     ]
    }
   ],
   "source": [
    "print('Out of {0} number of unknown words, modfified Viterbi has unable to correctly POS tagged {1} numbers of words.'\n",
    "      .format(len(unknown_words),len(incorrectly_tagged_unknown_words)))\n",
    "\n",
    "print('Incorrectly Tagged Unknownd Words are:\\n', list([i[0] for i in incorrectly_tagged_unknown_words ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasons for Incorrect Tagging:\n",
    "- There are few words for which the emission probability is much hgher in Train set than test set which is actually \n",
    "influencing the prediction on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "sentence_test = \"Android is a mobile operating system developed by Google. Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013. Google and Twitter made a deal in 2015 that gave Google access to Twitter's firehose. Twitter is an online news and social networking service on which users post and interact with messages known as tweets. Before entering politics, Donald Trump was a domineering businessman and a television personality. The 2018 FIFA World Cup is the 21st FIFA World Cup, an international football tournament contested once every four years. This is the first World Cup to be held in Eastern Europe and the 11th time that it has been held in Europe. Show me the cheapest round trips from Dallas to Atlanta.I would like to see flights from Denver to Philadelphia. Show me the price of the flights leaving Atlanta at about 3 in the afternoon and arriving in San Francisco. NASA invited social media users to experience the launch of ICESAT-2 Satellite.\"\n",
    "words = word_tokenize(sentence_test)\n",
    "\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'VERB'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google', 'VERB'), ('.', '.'), ('Android', 'VERB'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', 'VERB'), ('worldwide', 'VERB'), ('on', 'ADP'), ('smartphones', 'VERB'), ('since', 'ADP'), ('2011', 'VERB'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', 'VERB'), ('.', '.'), ('Google', 'VERB'), ('and', 'CONJ'), ('Twitter', 'VERB'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', 'VERB'), ('that', 'ADP'), ('gave', 'VERB'), ('Google', 'VERB'), ('access', 'NOUN'), ('to', 'PRT'), ('Twitter', 'VERB'), (\"'s\", 'PRT'), ('firehose', 'VERB'), ('.', '.'), ('Twitter', 'VERB'), ('is', 'VERB'), ('an', 'DET'), ('online', 'VERB'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', 'VERB'), ('with', 'ADP'), ('messages', 'VERB'), ('known', 'VERB'), ('as', 'ADP'), ('tweets', 'VERB'), ('.', '.'), ('Before', 'ADP'), ('entering', 'VERB'), ('politics', 'NOUN'), (',', '.'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', 'VERB'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality', 'VERB'), ('.', '.'), ('The', 'DET'), ('2018', 'VERB'), ('FIFA', 'VERB'), ('World', 'NOUN'), ('Cup', 'VERB'), ('is', 'VERB'), ('the', 'DET'), ('21st', 'VERB'), ('FIFA', 'VERB'), ('World', 'NOUN'), ('Cup', 'VERB'), (',', '.'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', 'VERB'), ('contested', 'VERB'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.'), ('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', 'VERB'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', 'VERB'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta.I', 'VERB'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', 'VERB'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco', 'NOUN'), ('.', '.'), ('NASA', 'VERB'), ('invited', 'VERB'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', 'VERB'), ('Satellite', 'VERB'), ('.', '.')]\n",
      "2.1635866165161133\n"
     ]
    }
   ],
   "source": [
    "print(tagged_seq)\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "tagged_seq = ViterbiForUnknownWords(words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('Android', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google', 'NOUN'), ('.', '.'), ('Android', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('on', 'ADP'), ('smartphones', 'NOUN'), ('since', 'ADP'), ('2011', 'NUM'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', 'NUM'), ('.', '.'), ('Google', 'NOUN'), ('and', 'CONJ'), ('Twitter', 'NOUN'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', 'NUM'), ('that', 'ADP'), ('gave', 'VERB'), ('Google', 'NOUN'), ('access', 'NOUN'), ('to', 'PRT'), ('Twitter', 'NOUN'), (\"'s\", 'PRT'), ('firehose', 'NOUN'), ('.', '.'), ('Twitter', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('online', 'NOUN'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', 'NOUN'), ('with', 'ADP'), ('messages', 'NOUN'), ('known', 'VERB'), ('as', 'ADP'), ('tweets', 'NOUN'), ('.', '.'), ('Before', 'ADP'), ('entering', 'VERB'), ('politics', 'NOUN'), (',', '.'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', 'VERB'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality', 'NOUN'), ('.', '.'), ('The', 'DET'), ('2018', 'NUM'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('21st', 'NOUN'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), (',', '.'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', 'NOUN'), ('contested', 'VERB'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.'), ('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', 'NOUN'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta.I', 'NOUN'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', 'VERB'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco', 'NOUN'), ('.', '.'), ('NASA', 'NOUN'), ('invited', 'VERB'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', 'NOUN'), ('Satellite', 'NOUN'), ('.', '.')], [('Android', 'NOUN'), ('Google', 'NOUN'), ('Android', 'NOUN'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('smartphones', 'NOUN'), ('2011', 'NUM'), ('2013', 'NUM'), ('Google', 'NOUN'), ('Twitter', 'NOUN'), ('2015', 'NUM'), ('Google', 'NOUN'), ('Twitter', 'NOUN'), ('firehose', 'NOUN'), ('Twitter', 'NOUN'), ('online', 'NOUN'), ('interact', 'NOUN'), ('messages', 'NOUN'), ('tweets', 'NOUN'), ('domineering', 'VERB'), ('personality', 'NOUN'), ('2018', 'NUM'), ('FIFA', 'NOUN'), ('Cup', 'NOUN'), ('21st', 'NOUN'), ('FIFA', 'NOUN'), ('Cup', 'NOUN'), ('tournament', 'NOUN'), ('contested', 'VERB'), ('Cup', 'NOUN'), ('trips', 'NOUN'), ('Atlanta.I', 'NOUN'), ('arriving', 'VERB'), ('NASA', 'NOUN'), ('invited', 'VERB'), ('ICESAT-2', 'NOUN'), ('Satellite', 'NOUN')])\n",
      "5.27733588218689\n"
     ]
    }
   ],
   "source": [
    "print(tagged_seq)\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "-  Modified Viterbi is performing well e.g. it is correctly Identifing the nouns : Android, smartphones and other tags as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
